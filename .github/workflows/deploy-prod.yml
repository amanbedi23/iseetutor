name: Deploy to Production

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      skip_terraform:
        description: 'Skip Terraform deployment'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: us-east-1
  ENVIRONMENT: prod
  CLUSTER_NAME: iseetutor-prod-cluster

jobs:
  approval:
    name: Production Deployment Approval
    runs-on: ubuntu-latest
    environment: production
    
    steps:
      - name: Request deployment approval
        run: echo "Deployment to production approved"

  backup-database:
    name: Backup Production Database
    needs: approval
    runs-on: ubuntu-latest
    
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Create RDS Snapshot
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          SNAPSHOT_ID="iseetutor-prod-pre-deploy-$TIMESTAMP"
          
          aws rds create-db-snapshot \
            --db-instance-identifier iseetutor-prod-db \
            --db-snapshot-identifier $SNAPSHOT_ID
          
          echo "SNAPSHOT_ID=$SNAPSHOT_ID" >> $GITHUB_ENV
      
      - name: Wait for snapshot completion
        run: |
          aws rds wait db-snapshot-completed \
            --db-snapshot-identifier ${{ env.SNAPSHOT_ID }}

  deploy-infrastructure:
    name: Deploy Infrastructure
    needs: backup-database
    runs-on: ubuntu-latest
    if: ${{ !inputs.skip_terraform }}
    outputs:
      alb_dns: ${{ steps.terraform-output.outputs.alb_dns }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0
          terraform_wrapper: false
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Terraform Init
        working-directory: terraform/environments/prod
        run: terraform init
      
      - name: Terraform Plan
        working-directory: terraform/environments/prod
        env:
          TF_VAR_openai_api_key: ${{ secrets.PROD_OPENAI_API_KEY }}
          TF_VAR_google_cloud_key: ${{ secrets.PROD_GOOGLE_CLOUD_KEY }}
          TF_VAR_pinecone_api_key: ${{ secrets.PROD_PINECONE_API_KEY }}
          TF_VAR_aws_access_key_id_for_services: ${{ secrets.PROD_AWS_ACCESS_KEY_ID_FOR_SERVICES }}
          TF_VAR_aws_secret_access_key_for_services: ${{ secrets.PROD_AWS_SECRET_ACCESS_KEY_FOR_SERVICES }}
          TF_VAR_backend_image_tag: ${{ github.sha }}
          TF_VAR_frontend_image_tag: ${{ github.sha }}
          TF_VAR_alarm_email: ${{ secrets.PROD_ALARM_EMAIL }}
          TF_VAR_domain_name: ${{ secrets.PROD_DOMAIN_NAME }}
          TF_VAR_certificate_arn: ${{ secrets.PROD_CERTIFICATE_ARN }}
        run: terraform plan -out=tfplan
      
      - name: Terraform Apply
        working-directory: terraform/environments/prod
        env:
          TF_VAR_openai_api_key: ${{ secrets.PROD_OPENAI_API_KEY }}
          TF_VAR_google_cloud_key: ${{ secrets.PROD_GOOGLE_CLOUD_KEY }}
          TF_VAR_pinecone_api_key: ${{ secrets.PROD_PINECONE_API_KEY }}
          TF_VAR_aws_access_key_id_for_services: ${{ secrets.PROD_AWS_ACCESS_KEY_ID_FOR_SERVICES }}
          TF_VAR_aws_secret_access_key_for_services: ${{ secrets.PROD_AWS_SECRET_ACCESS_KEY_FOR_SERVICES }}
          TF_VAR_backend_image_tag: ${{ github.sha }}
          TF_VAR_frontend_image_tag: ${{ github.sha }}
          TF_VAR_alarm_email: ${{ secrets.PROD_ALARM_EMAIL }}
          TF_VAR_domain_name: ${{ secrets.PROD_DOMAIN_NAME }}
          TF_VAR_certificate_arn: ${{ secrets.PROD_CERTIFICATE_ARN }}
        run: terraform apply tfplan
      
      - name: Get Terraform outputs
        id: terraform-output
        working-directory: terraform/environments/prod
        run: |
          echo "alb_dns=$(terraform output -raw alb_dns_name)" >> $GITHUB_OUTPUT

  deploy-backend-canary:
    name: Deploy Backend (Canary)
    needs: deploy-infrastructure
    runs-on: ubuntu-latest
    
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update ECS Backend Service (10% traffic)
        run: |
          # Update task definition
          TASK_DEF_ARN=$(aws ecs describe-task-definition \
            --task-definition iseetutor-prod-backend \
            --query 'taskDefinition.taskDefinitionArn' \
            --output text)
          
          # Update service with canary deployment
          aws ecs update-service \
            --cluster ${{ env.CLUSTER_NAME }} \
            --service iseetutor-prod-backend \
            --task-definition $TASK_DEF_ARN \
            --deployment-configuration "maximumPercent=200,minimumHealthyPercent=100,deploymentCircuitBreaker={enable=true,rollback=true}" \
            --desired-count 4
      
      - name: Monitor Canary Deployment
        run: |
          sleep 300  # Wait 5 minutes
          
          # Check CloudWatch alarms
          ALARMS=$(aws cloudwatch describe-alarms \
            --alarm-name-prefix "iseetutor-prod-" \
            --state-value ALARM \
            --query 'MetricAlarms[].AlarmName' \
            --output text)
          
          if [ -n "$ALARMS" ]; then
            echo "Alarms triggered during canary deployment: $ALARMS"
            exit 1
          fi

  deploy-backend-full:
    name: Deploy Backend (Full)
    needs: deploy-backend-canary
    runs-on: ubuntu-latest
    
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Complete Backend Deployment
        run: |
          aws ecs update-service \
            --cluster ${{ env.CLUSTER_NAME }} \
            --service iseetutor-prod-backend \
            --force-new-deployment
      
      - name: Wait for Backend Deployment
        run: |
          aws ecs wait services-stable \
            --cluster ${{ env.CLUSTER_NAME }} \
            --services iseetutor-prod-backend

  deploy-frontend:
    name: Deploy Frontend Service
    needs: deploy-backend-full
    runs-on: ubuntu-latest
    
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update ECS Frontend Service
        run: |
          aws ecs update-service \
            --cluster ${{ env.CLUSTER_NAME }} \
            --service iseetutor-prod-frontend \
            --force-new-deployment
      
      - name: Wait for Frontend Deployment
        run: |
          aws ecs wait services-stable \
            --cluster ${{ env.CLUSTER_NAME }} \
            --services iseetutor-prod-frontend

  run-migrations:
    name: Run Database Migrations
    needs: [deploy-backend-full]
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Run Database Migrations
        run: |
          # Similar to dev, but for prod environment
          SUBNET_ID=$(aws ec2 describe-subnets \
            --filters "Name=tag:Name,Values=iseetutor-prod-private-subnet-1" \
            --query 'Subnets[0].SubnetId' \
            --output text)
          
          SECURITY_GROUP_ID=$(aws ec2 describe-security-groups \
            --filters "Name=tag:Name,Values=iseetutor-prod-ecs-tasks-sg" \
            --query 'SecurityGroups[0].GroupId' \
            --output text)
          
          # Run migration task
          TASK_ARN=$(aws ecs run-task \
            --cluster ${{ env.CLUSTER_NAME }} \
            --task-definition iseetutor-prod-backend \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[$SUBNET_ID],securityGroups=[$SECURITY_GROUP_ID]}" \
            --overrides '{"containerOverrides":[{"name":"backend","command":["alembic","upgrade","head"]}]}' \
            --query 'tasks[0].taskArn' \
            --output text)
          
          aws ecs wait tasks-stopped --cluster ${{ env.CLUSTER_NAME }} --tasks $TASK_ARN
          
          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster ${{ env.CLUSTER_NAME }} \
            --tasks $TASK_ARN \
            --query 'tasks[0].containers[0].exitCode' \
            --output text)
          
          if [ "$EXIT_CODE" != "0" ]; then
            echo "Migration failed with exit code $EXIT_CODE"
            exit 1
          fi

  smoke-tests:
    name: Run Production Smoke Tests
    needs: [deploy-frontend, run-migrations]
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Production Health Checks
        run: |
          DOMAIN="${{ secrets.PROD_DOMAIN_NAME }}"
          
          # Test health endpoint
          curl -s -f "https://${DOMAIN}/health" || exit 1
          
          # Test API availability
          curl -s -f "https://${DOMAIN}/docs" || exit 1
          
          # Test frontend
          curl -s -f "https://${DOMAIN}" || exit 1
      
      - name: Run E2E Tests
        run: |
          # Run subset of E2E tests against production
          npm install -g @playwright/test
          npx playwright test tests/e2e/smoke.spec.ts --project=production

  rollback:
    name: Rollback if Failed
    needs: [smoke-tests]
    runs-on: ubuntu-latest
    if: failure()
    
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Rollback ECS Services
        run: |
          # Get previous task definition
          PREV_BACKEND=$(aws ecs describe-services \
            --cluster ${{ env.CLUSTER_NAME }} \
            --services iseetutor-prod-backend \
            --query 'services[0].deployments[1].taskDefinition' \
            --output text)
          
          PREV_FRONTEND=$(aws ecs describe-services \
            --cluster ${{ env.CLUSTER_NAME }} \
            --services iseetutor-prod-frontend \
            --query 'services[0].deployments[1].taskDefinition' \
            --output text)
          
          # Rollback services
          aws ecs update-service \
            --cluster ${{ env.CLUSTER_NAME }} \
            --service iseetutor-prod-backend \
            --task-definition $PREV_BACKEND \
            --force-new-deployment
          
          aws ecs update-service \
            --cluster ${{ env.CLUSTER_NAME }} \
            --service iseetutor-prod-frontend \
            --task-definition $PREV_FRONTEND \
            --force-new-deployment
      
      - name: Notify rollback
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              text: "ðŸš¨ Production deployment failed and was rolled back!",
              attachments: [{
                color: 'danger',
                fields: [
                  { title: 'Environment', value: 'Production', short: true },
                  { title: 'Commit', value: '${{ github.sha }}', short: true },
                  { title: 'Author', value: '${{ github.actor }}', short: true },
                  { title: 'Action', value: 'Automatic Rollback', short: true }
                ]
              }]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  notify-success:
    name: Notify Success
    needs: [smoke-tests]
    runs-on: ubuntu-latest
    
    steps:
      - name: Send success notification
        uses: 8398a7/action-slack@v3
        with:
          status: custom
          custom_payload: |
            {
              text: "âœ… Production deployment successful!",
              attachments: [{
                color: 'good',
                fields: [
                  { title: 'Environment', value: 'Production', short: true },
                  { title: 'Version', value: '${{ github.sha }}', short: true },
                  { title: 'URL', value: 'https://${{ secrets.PROD_DOMAIN_NAME }}', short: true },
                  { title: 'Deployed by', value: '${{ github.actor }}', short: true }
                ]
              }]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      
      - name: Create deployment record
        uses: actions/github-script@v7
        with:
          script: |
            const deployment = await github.rest.repos.createDeployment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              ref: context.sha,
              environment: 'production',
              required_contexts: [],
              auto_merge: false,
              payload: {
                version: context.sha,
                url: 'https://${{ secrets.PROD_DOMAIN_NAME }}'
              }
            });
            
            await github.rest.repos.createDeploymentStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              deployment_id: deployment.data.id,
              state: 'success',
              environment_url: 'https://${{ secrets.PROD_DOMAIN_NAME }}'
            });